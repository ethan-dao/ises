{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('../data/test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('../data/trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.train_len = train_len\n",
    "        self.seq_len = seq_len\n",
    "        #self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        self.W_Q = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_K = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_V = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "\n",
    "        \n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(self.attention_size, self.hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        #layers.append(nn.Dropout(self.dropout_rate))\n",
    "        \n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            #layers.append(nn.Dropout(self.dropout_rate)) #Add this later on\n",
    "\n",
    "        layers.append(nn.Linear(self.hidden_size, 1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.criterion = nn.MSELoss() # Switch to mean squared error instead of simple norm (this is better apparently?)\n",
    "\n",
    "    def loss(self, predicted, y):\n",
    "        return torch.norm(predicted - y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of size                                               (batch_size, sequence_length, embedding_size)\n",
    "        if x.shape[-1] != self.embed_size:\n",
    "            raise ValueError\n",
    "\n",
    "        queries = self.W_Q(x) #                                   (batch_size, sequence_length, attention_size)\n",
    "        keys = self.W_K(x) #                                      (batch_size, sequence_length, attention_size)\n",
    "        values = self.W_V(x) #                                    (batch_size, sequence_length, attention_size)\n",
    "\n",
    "        # Scale to prevent overflow errors, divide by square root of attention dimension\n",
    "        scale = torch.sqrt(torch.tensor(self.attention_size, dtype=torch.float32))\n",
    "\n",
    "        #Compute attention and then normalize\n",
    "        attention = torch.bmm(queries, keys.transpose(1,2)) / scale #                  (batch_size, seq_len, seq_len)\n",
    "        weights = torch.nn.functional.softmax(attention, dim=2) # Apply this per sample\n",
    "\n",
    "        # Use as weights for values\n",
    "        context = torch.bmm(weights, values) #    (batch_size, attention_size, sequence_length)\n",
    "        # Run through all FFN layers\n",
    "        for layer in self.layers:\n",
    "            context = layer(context)\n",
    "        # Return prediction with added b term\n",
    "        return context.squeeze(-1)\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) # This is for stability\n",
    "        self.optimizer.step()\n",
    "        return loss.item() # Diagnostic info\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        losses = []\n",
    "        for epoch in range(self.train_len):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.train_len}, Loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/35, Loss: 0.3566\n",
      "Epoch 10/35, Loss: 0.3556\n",
      "Epoch 15/35, Loss: 0.3550\n",
      "Epoch 20/35, Loss: 0.3545\n",
      "Epoch 25/35, Loss: 0.3546\n",
      "Epoch 30/35, Loss: 0.3547\n",
      "Epoch 35/35, Loss: 0.3552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3722450732588768,\n",
       " 0.36231695806980135,\n",
       " 0.35980509281158446,\n",
       " 0.35866012406349185,\n",
       " 0.35661341854929923,\n",
       " 0.3559960229992867,\n",
       " 0.359088103979826,\n",
       " 0.35603070840239526,\n",
       " 0.3574793221652508,\n",
       " 0.355563128978014,\n",
       " 0.35619551116228104,\n",
       " 0.3593996531069279,\n",
       " 0.35545326939225197,\n",
       " 0.35616158944368365,\n",
       " 0.3550125886499882,\n",
       " 0.35506059423089026,\n",
       " 0.3565283133685589,\n",
       " 0.3555122908055782,\n",
       " 0.3553130387365818,\n",
       " 0.3545332871377468,\n",
       " 0.35446419486403463,\n",
       " 0.3545258333384991,\n",
       " 0.35608106151223184,\n",
       " 0.35420405626296997,\n",
       " 0.35460475540161135,\n",
       " 0.3538242387771606,\n",
       " 0.3551491311788559,\n",
       " 0.35666617238521575,\n",
       " 0.3551843306422234,\n",
       " 0.3546984785795212,\n",
       " 0.3567810878455639,\n",
       " 0.3567938947081566,\n",
       " 0.35445902952551844,\n",
       " 0.35529606401920316,\n",
       " 0.3552296207249165]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = DNADataset(embedded_dna_strands_train, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(20, 295, 5, 10, 1, 1e-4, 35) # (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02421159, -0.00877821, -0.00672451, ..., -0.01295421,\n",
       "        -0.01295421, -0.01295421],\n",
       "       [-0.02848384, -0.0219145 , -0.02493575, ..., -0.03607342,\n",
       "        -0.03607342, -0.03607342],\n",
       "       [-0.03253882, -0.01874664, -0.0211546 , ..., -0.03607342,\n",
       "        -0.03607342, -0.03607342],\n",
       "       ...,\n",
       "       [-0.02467053, -0.00340417, -0.00290501, ..., -0.01295421,\n",
       "        -0.01295421, -0.01295421],\n",
       "       [-0.02452026, -0.00872271, -0.00896282, ...,  0.00754449,\n",
       "         0.00754488,  0.00754488],\n",
       "       [-0.03271075, -0.01227978, -0.01104505, ...,  0.00754488,\n",
       "         0.00754488,  0.00754488]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nucleotide_vals = model.forward(torch.Tensor(embedded_dna_strands_test)).detach().numpy()\n",
    "nucleotide_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "flat_vals = nucleotide_vals.flatten()\n",
    "top_100k_indices = np.argpartition(flat_vals, -100000)[-100000:] # Get top 100k indices and sort (sort and get indices using argpartition)\n",
    "top_100k_indices_sorted = top_100k_indices[np.argsort(flat_vals[top_100k_indices])[::-1]]\n",
    "\n",
    "bottom_50k_indices = np.argpartition(flat_vals, 50000)[:50000]  # Get bottom 50k indices and sort\n",
    "bottom_50k_indices_sorted = bottom_50k_indices[np.argsort(flat_vals[bottom_50k_indices])]  \n",
    "\n",
    "top_100k_coords = [divmod(index, nucleotide_vals.shape[1]) for index in top_100k_indices_sorted]\n",
    "bottom_50k_coords = [divmod(index, nucleotide_vals.shape[1]) for index in bottom_50k_indices_sorted]\n",
    "\n",
    "max_row = max(max(row for row, _ in top_100k_coords), \n",
    "              max(row for row, _ in bottom_50k_coords))\n",
    "pad_width = len(str(max_row))\n",
    "\n",
    "for row, col in top_100k_coords:\n",
    "    output.append([\"A\", f\"test{str(row).zfill(pad_width)}\", f\"{col}\"])\n",
    "for row, col in bottom_50k_coords:\n",
    "    output.append([\"R\", f\"test{str(row).zfill(pad_width)}\", f\"{col}\"])\n",
    "\n",
    "output_df = pd.DataFrame(output, columns = [\"Activation/Repression\", \"SequenceID\", \"Nucleotide\"])\n",
    "output_df.to_csv(\"predictions.txt\", sep=\"\\t\", header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The output should now reflect predictions based on the training data\n",
    "\n",
    "#Generate predictions\n",
    "predictions = []\n",
    "for i, strand in enumerate(embedded_dna_strands_train):\n",
    "    logits = model.forward(torch.Tensor(strand).unsqueeze(0)).detach().numpy().flatten()\n",
    "    predicted_scores = torch.sigmoid(torch.Tensor(logits)).numpy().flatten()  #Apply Sigmoid to logits\n",
    "    predicted_labels = [\"A\" if score > 0.5 else \"R\" for score in predicted_scores]  #0.5 threshold\n",
    "    sequence_id = f\"train{str(i).zfill(4)}\"\n",
    "    nucleotides = raw_dna_strands_train[i]\n",
    "\n",
    "    for pos, (nucleotide, label) in enumerate(zip(nucleotides, predicted_labels)):\n",
    "        predictions.append([label, sequence_id, nucleotide])\n",
    "\n",
    "#Save predictions\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "predictions_df.to_csv(\"predictions.txt\", sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "#Load training solutions\n",
    "training_solutions = pd.read_csv(\"../data/trainsolutions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Load predictions\n",
    "predictions = pd.read_csv(\"predictions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Ensure \"Nucleotide\" columns are strings in both DataFrames\n",
    "predictions[\"Nucleotide\"] = predictions[\"Nucleotide\"].astype(str)\n",
    "training_solutions[\"Nucleotide\"] = training_solutions[\"Nucleotide\"].astype(str)\n",
    "\n",
    "#Merge predictions and training solutions\n",
    "comparison = pd.merge(predictions, training_solutions, on=[\"Sequence_ID\", \"Nucleotide\"], how=\"inner\", suffixes=('_pred', '_true'))\n",
    "\n",
    "#Add a \"Match\" column to indicate where Prediction matches Truth\n",
    "comparison[\"Match\"] = comparison[\"Activation/Repression_pred\"] == comparison[\"Activation/Repression_true\"]\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = comparison[\"Match\"].mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "#Save the comparison to a file for inspection\n",
    "comparison.to_csv(\"comparison.txt\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
