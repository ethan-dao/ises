{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('../data/test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('../data/trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.train_len = train_len\n",
    "        self.seq_len = seq_len\n",
    "        #self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        self.W_Q = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_K = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_V = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "\n",
    "        \n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(self.attention_size, self.hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        #layers.append(nn.Dropout(self.dropout_rate))\n",
    "        \n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            #layers.append(nn.Dropout(self.dropout_rate)) #Add this later on\n",
    "\n",
    "        layers.append(nn.Linear(self.hidden_size, 1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.criterion = nn.MSELoss() # Switch to mean squared error instead of simple norm (this is better apparently?)\n",
    "\n",
    "    def loss(self, predicted, y):\n",
    "        return torch.norm(predicted - y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of size                                               (batch_size, sequence_length, embedding_size)\n",
    "        if x.shape[-1] != self.embed_size:\n",
    "            raise ValueError\n",
    "\n",
    "        queries = self.W_Q(x) #                                   (batch_size, sequence_length, attention_size)\n",
    "        keys = self.W_K(x) #                                      (batch_size, sequence_length, attention_size)\n",
    "        values = self.W_V(x) #                                    (batch_size, sequence_length, attention_size)\n",
    "\n",
    "        # Scale to prevent overflow errors, divide by square root of attention dimension\n",
    "        scale = torch.sqrt(torch.tensor(self.attention_size, dtype=torch.float32))\n",
    "\n",
    "        #Compute attention and then normalize\n",
    "        attention = torch.bmm(queries, keys.transpose(1,2)) / scale #                  (batch_size, seq_len, seq_len)\n",
    "        weights = torch.nn.functional.softmax(attention, dim=2) # Apply this per sample\n",
    "\n",
    "        # Use as weights for values\n",
    "        context = torch.bmm(weights, values) #    (batch_size, attention_size, sequence_length)\n",
    "        # Run through all FFN layers\n",
    "        for layer in self.layers:\n",
    "            context = layer(context)\n",
    "        # Return prediction with added b term\n",
    "        return context.squeeze(-1)\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) # This is for stability\n",
    "        self.optimizer.step()\n",
    "        return loss.item() # Diagnostic info\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        losses = []\n",
    "        for epoch in range(self.train_len):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.train_len}, Loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.3582\n",
      "Epoch 10/100, Loss: 0.3545\n",
      "Epoch 15/100, Loss: 0.3546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m SelfAttentionFeedForward(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m295\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 91\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     89\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     93\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "Cell \u001b[0;32mIn[21], line 79\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, y)\n\u001b[1;32m     81\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[21], line 70\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Apply this per sample\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Use as weights for values\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#    (batch_size, attention_size, sequence_length)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Run through all FFN layers\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = DNADataset(embedded_dna_strands_train, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(20, 295, 5, 10, 1, 1e-4, 100) # (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00706012, -0.02109713, -0.01993171, ..., -0.05370706,\n",
       "        -0.05370611, -0.05370808],\n",
       "       [ 0.12066531,  0.11658909,  0.11174172, ..., -0.22372211,\n",
       "        -0.22372211, -0.22372211],\n",
       "       [ 0.10770184,  0.09660789,  0.08575717, ..., -0.02368841,\n",
       "        -0.02369127, -0.02369177],\n",
       "       ...,\n",
       "       [-0.0144823 , -0.02773503, -0.03449617, ..., -0.06272289,\n",
       "        -0.06272328, -0.06272205],\n",
       "       [ 0.0398432 ,  0.02662977,  0.02777308, ..., -0.01613432,\n",
       "        -0.01613432, -0.01613432],\n",
       "       [-0.13925827, -0.16049814, -0.16456118, ..., -0.06164312,\n",
       "        -0.06160202, -0.06170475]], shape=(7998, 295))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.Tensor(embedded_dna_strands_train)).detach().numpy() - train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The output should now reflect predictions based on the training data\n",
    "\n",
    "#Generate predictions\n",
    "predictions = []\n",
    "for i, strand in enumerate(embedded_dna_strands_train):\n",
    "    logits = model.forward(torch.Tensor(strand).unsqueeze(0)).detach().numpy().flatten()\n",
    "    predicted_scores = torch.sigmoid(torch.Tensor(logits)).numpy().flatten()  #Apply Sigmoid to logits\n",
    "    predicted_labels = [\"A\" if score > 0.5 else \"R\" for score in predicted_scores]  #0.5 threshold\n",
    "    sequence_id = f\"train{str(i).zfill(4)}\"\n",
    "    nucleotides = raw_dna_strands_train[i]\n",
    "\n",
    "    for pos, (nucleotide, label) in enumerate(zip(nucleotides, predicted_labels)):\n",
    "        predictions.append([label, sequence_id, nucleotide])\n",
    "\n",
    "#Save predictions\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "predictions_df.to_csv(\"predictions.txt\", sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "#Load training solutions\n",
    "training_solutions = pd.read_csv(\"../data/trainsolutions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Load predictions\n",
    "predictions = pd.read_csv(\"predictions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Ensure \"Nucleotide\" columns are strings in both DataFrames\n",
    "predictions[\"Nucleotide\"] = predictions[\"Nucleotide\"].astype(str)\n",
    "training_solutions[\"Nucleotide\"] = training_solutions[\"Nucleotide\"].astype(str)\n",
    "\n",
    "#Merge predictions and training solutions\n",
    "comparison = pd.merge(predictions, training_solutions, on=[\"Sequence_ID\", \"Nucleotide\"], how=\"inner\", suffixes=('_pred', '_true'))\n",
    "\n",
    "#Add a \"Match\" column to indicate where Prediction matches Truth\n",
    "comparison[\"Match\"] = comparison[\"Activation/Repression_pred\"] == comparison[\"Activation/Repression_true\"]\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = comparison[\"Match\"].mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "#Save the comparison to a file for inspection\n",
    "comparison.to_csv(\"comparison.txt\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
