{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import SelfAttentionFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('../data/test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('../data/trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, embed_size, hidden_size, hidden_layers, lr, train_len):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.train_len = train_len\n",
    "        #self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        self.W_Q = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_K = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.W_V = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
    "        self.b = nn.Parameter(torch.rand(295)) # This is an addition term, analogous to y-intercept\n",
    "\n",
    "        \n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(self.attention_size, self.hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        #layers.append(nn.Dropout(self.dropout_rate))\n",
    "        \n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            #layers.append(nn.Dropout(self.dropout_rate)) #Add this later on\n",
    "\n",
    "        layers.append(nn.Linear(self.hidden_size, 1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.criterion = nn.MSELoss() # Swithc to mean squared error instead of simple norm (this is better apparently?)\n",
    "\n",
    "    def loss(self, predicted, y):\n",
    "        return torch.norm(predicted - y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of size                                               (batch_size, sequence_length, embedding_size)\n",
    "        batch_size, seq_len, emb_size = x.shape\n",
    "        if emb_size != self.embed_size:\n",
    "            raise ValueError\n",
    "\n",
    "        queries = self.W_Q(x) #                                   (batch_size, sequence_length, attention_size)\n",
    "        keys = self.W_K(x) #                                      (batch_size, sequence_length, attention_size)\n",
    "        values = self.W_V(x) #                                    (batch_size, sequence_length, attention_size)\n",
    "\n",
    "        #Compute attention and then normalize\n",
    "        attention = torch.bmm(queries.transpose(1,2), keys) #                  (batch_size, attention_size, attention_size)\n",
    "        weights = torch.nn.functional.softmax(attention, dim=2) # Apply this per sample\n",
    "\n",
    "        # Use as weights for values\n",
    "        context = torch.bmm(weights, values.transpose(1,2)).transpose(1,2) #    (batch_size, attention_size, sequence_length)\n",
    "        # Run through all FFN layers\n",
    "        for layer in self.layers:\n",
    "            context = layer(context)\n",
    "        # Return prediction with added b term\n",
    "        return context\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred.squeeze(-1), y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) # This is for stability\n",
    "        self.optimizer.step()\n",
    "        return loss.item() # Diagnostic info\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        losses = []\n",
    "        for epoch in range(self.train_len):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.train_len}, Loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.3679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = DNADataset(embedded_dna_strands_train, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(50, 5, 20, 3, 1e-5, 100) # (attention_size, embed_size, hidden_size, hidden_layers, lr, train_len)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22174352,  0.71485429,  0.62327915, ...,  1.0669322 ,\n",
       "         0.41677427,  1.01910806],\n",
       "       [ 0.27477124,  0.76788204,  0.6763069 , ...,  1.11995983,\n",
       "         0.46980199,  1.07213569],\n",
       "       [ 0.34722375,  0.84033453,  0.74875938, ...,  1.19241238,\n",
       "         0.54225451,  1.14458823],\n",
       "       ...,\n",
       "       [19.20031839, 19.69342905, 19.60185342, ..., 20.04550743,\n",
       "        19.3953495 , 19.99768257],\n",
       "       [19.26924997, 19.76236063, 19.670785  , ..., 20.11443901,\n",
       "        19.46428108, 20.06661415],\n",
       "       [19.33420473, 19.82731538, 19.73573976, ..., 20.17939377,\n",
       "        19.52923584, 20.13156891]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(torch.Tensor(embedded_dna_strands_train[0])).detach().numpy() - train_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.019,  0.019,  0.019,  0.018,  0.017,  0.017,  0.016,  0.015,\n",
       "        0.037,  0.059,  0.082,  0.104,  0.126,  0.125,  0.125,  0.124,\n",
       "        0.124,  0.123,  0.116,  0.109,  0.102,  0.095,  0.088,  0.07 ,\n",
       "        0.053,  0.035,  0.018,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.005,  0.01 ,  0.014,  0.019,  0.024,  0.019,  0.014,\n",
       "        0.01 ,  0.005,  0.   , -0.052, -0.104, -0.156, -0.208, -0.26 ,\n",
       "       -0.27 , -0.28 , -0.29 , -0.3  , -0.31 , -0.298, -0.286, -0.275,\n",
       "       -0.263, -0.251, -0.253, -0.255, -0.258, -0.26 , -0.262, -0.248,\n",
       "       -0.234, -0.219, -0.205, -0.191, -0.202, -0.213, -0.223, -0.234,\n",
       "       -0.245, -0.235, -0.225, -0.214, -0.204, -0.194, -0.191, -0.188,\n",
       "       -0.185, -0.182, -0.179, -0.181, -0.183, -0.184, -0.186, -0.188,\n",
       "       -0.155, -0.122, -0.089, -0.056, -0.023, -0.007,  0.009,  0.025,\n",
       "        0.041,  0.057,  0.086,  0.116,  0.145,  0.175,  0.204,  0.21 ,\n",
       "        0.216,  0.222,  0.228,  0.234,  0.24 ,  0.246,  0.252,  0.258,\n",
       "        0.264,  0.255,  0.246,  0.236,  0.227,  0.218,  0.231,  0.244,\n",
       "        0.256,  0.269,  0.282,  0.268,  0.254,  0.239,  0.225,  0.211,\n",
       "        0.224,  0.236,  0.249,  0.261,  0.274,  0.277,  0.28 ,  0.282,\n",
       "        0.285,  0.288,  0.237,  0.186,  0.135,  0.084,  0.033,  0.077,\n",
       "        0.12 ,  0.164,  0.207,  0.251,  0.237,  0.223,  0.208,  0.194,\n",
       "        0.18 ,  0.144,  0.108,  0.072,  0.036,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.037,  0.074,  0.11 ,  0.147,  0.184,  0.18 ,  0.175,  0.171,\n",
       "        0.166,  0.162,  0.13 ,  0.097,  0.065,  0.032,  0.   ,  0.003,\n",
       "        0.007,  0.01 ,  0.014,  0.017,  0.141,  0.266,  0.39 ,  0.515,\n",
       "        0.639,  0.649,  0.659,  0.669,  0.679,  0.689,  0.677,  0.665,\n",
       "        0.654,  0.642,  0.63 ,  0.632,  0.634,  0.637,  0.639,  0.641,\n",
       "        0.627,  0.613,  0.598,  0.584,  0.57 ,  0.581,  0.592,  0.603,\n",
       "        0.614,  0.625,  0.615,  0.604,  0.594,  0.583,  0.573,  0.57 ,\n",
       "        0.567,  0.564,  0.561,  0.558,  0.56 ,  0.562,  0.564,  0.566,\n",
       "        0.568,  0.531,  0.494,  0.458,  0.421,  0.384,  0.344,  0.303,\n",
       "        0.263,  0.222,  0.182,  0.146,  0.109,  0.073,  0.036,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.008,  0.017,  0.025,  0.034,  0.042,  0.034,\n",
       "        0.025,  0.017,  0.008,  0.   ,  0.018,  0.036,  0.054,  0.072,\n",
       "        0.09 ,  0.077,  0.064,  0.052,  0.039,  0.026,  0.023,  0.02 ,\n",
       "        0.018,  0.015,  0.012,  0.029,  0.046,  0.062,  0.079,  0.096,\n",
       "        0.077,  0.058,  0.038,  0.019,  0.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
