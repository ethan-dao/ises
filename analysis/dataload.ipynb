{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"N\" in raw_dna_strands_train[3356]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'T',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'C',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'C',\n",
       " 'T',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'T',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'T',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'C',\n",
       " 'G',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'T',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'C',\n",
       " 'A',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'G',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'T',\n",
       " 'A',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'C',\n",
       " 'T',\n",
       " 'C',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'T',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'C',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'A',\n",
       " 'C',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'T',\n",
       " 'T',\n",
       " 'C',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'T',\n",
       " 'G',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'G',\n",
       " 'A']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dna_strands_train[3356]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, embed_size, hidden_size, hidden_layers, lr):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        self.W_Q = nn.Parameter(torch.rand(self.attention_size, self.embed_size))\n",
    "        self.W_K = nn.Parameter(torch.rand(self.attention_size, self.embed_size))\n",
    "        self.W_V = nn.Parameter(torch.rand(self.attention_size, self.embed_size))\n",
    "        self.b = nn.Parameter(torch.rand(295)) # This is an addition term, analogous to y-intercept\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "        fc1 = nn.Linear(self.attention_size, self.hidden_size)\n",
    "        relu1 = nn.ReLU()\n",
    "        self.layers = [fc1, relu1]\n",
    "        for i in range(self.hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(self.hidden_size, 1))\n",
    "\n",
    "\n",
    "    def loss(self, predicted, y):\n",
    "        return torch.norm(predicted - y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Multiply by all embeddings\n",
    "        queries = torch.matmul(self.W_Q, x.T)\n",
    "        keys = torch.matmul(self.W_K, x.T)\n",
    "        values = torch.matmul(self.W_V, x.T)\n",
    "\n",
    "        #Compute attention and then normalize\n",
    "        attention = torch.matmul(queries, keys.T)\n",
    "        weights = torch.nn.functional.softmax(attention, dim=0)\n",
    "\n",
    "        # Use as weights for values\n",
    "        contextualized_embeddings = torch.matmul(weights, values).T #This is dim self.attention_size x 295\n",
    "        # Run through all FFN layers\n",
    "        for layer in self.layers:\n",
    "            contextualized_embeddings = layer(contextualized_embeddings)\n",
    "        # Return prediction with added b term\n",
    "        return contextualized_embeddings + self.b\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        pred = self.predict(x)\n",
    "        loss = self.loss(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return None\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            self.train_step(x_batch, y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/h70r_yzx4qxdpyxrq5f91p7c0000gn/T/ipykernel_55812/1527061728.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (160x295 and 5x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m SelfAttentionFeedForward(\u001b[39m50\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1e-5\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m model\u001b[39m.\u001b[39mtrain(dataloader)\n",
      "Cell \u001b[0;32mIn[44], line 81\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, dataloader):\n\u001b[1;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step(x_batch, y_batch)\n",
      "Cell \u001b[0;32mIn[44], line 72\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 72\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x)\n\u001b[1;32m     73\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(pred, y)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[44], line 55\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m     \u001b[39m# Multiply by all embeddings\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     queries \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_Q, x\u001b[39m.\u001b[39mT)\n\u001b[1;32m     56\u001b[0m     keys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_K, x\u001b[39m.\u001b[39mT)\n\u001b[1;32m     57\u001b[0m     values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_V, x\u001b[39m.\u001b[39mT)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (160x295 and 5x50)"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = DNADataset(embedded_dna_strands_train, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(50, 5, 20, 3, 1e-5)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22174352,  0.71485429,  0.62327915, ...,  1.0669322 ,\n",
       "         0.41677427,  1.01910806],\n",
       "       [ 0.27477124,  0.76788204,  0.6763069 , ...,  1.11995983,\n",
       "         0.46980199,  1.07213569],\n",
       "       [ 0.34722375,  0.84033453,  0.74875938, ...,  1.19241238,\n",
       "         0.54225451,  1.14458823],\n",
       "       ...,\n",
       "       [19.20031839, 19.69342905, 19.60185342, ..., 20.04550743,\n",
       "        19.3953495 , 19.99768257],\n",
       "       [19.26924997, 19.76236063, 19.670785  , ..., 20.11443901,\n",
       "        19.46428108, 20.06661415],\n",
       "       [19.33420473, 19.82731538, 19.73573976, ..., 20.17939377,\n",
       "        19.52923584, 20.13156891]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(torch.Tensor(embedded_dna_strands_train[0])).detach().numpy() - train_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.019,  0.019,  0.019,  0.018,  0.017,  0.017,  0.016,  0.015,\n",
       "        0.037,  0.059,  0.082,  0.104,  0.126,  0.125,  0.125,  0.124,\n",
       "        0.124,  0.123,  0.116,  0.109,  0.102,  0.095,  0.088,  0.07 ,\n",
       "        0.053,  0.035,  0.018,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.005,  0.01 ,  0.014,  0.019,  0.024,  0.019,  0.014,\n",
       "        0.01 ,  0.005,  0.   , -0.052, -0.104, -0.156, -0.208, -0.26 ,\n",
       "       -0.27 , -0.28 , -0.29 , -0.3  , -0.31 , -0.298, -0.286, -0.275,\n",
       "       -0.263, -0.251, -0.253, -0.255, -0.258, -0.26 , -0.262, -0.248,\n",
       "       -0.234, -0.219, -0.205, -0.191, -0.202, -0.213, -0.223, -0.234,\n",
       "       -0.245, -0.235, -0.225, -0.214, -0.204, -0.194, -0.191, -0.188,\n",
       "       -0.185, -0.182, -0.179, -0.181, -0.183, -0.184, -0.186, -0.188,\n",
       "       -0.155, -0.122, -0.089, -0.056, -0.023, -0.007,  0.009,  0.025,\n",
       "        0.041,  0.057,  0.086,  0.116,  0.145,  0.175,  0.204,  0.21 ,\n",
       "        0.216,  0.222,  0.228,  0.234,  0.24 ,  0.246,  0.252,  0.258,\n",
       "        0.264,  0.255,  0.246,  0.236,  0.227,  0.218,  0.231,  0.244,\n",
       "        0.256,  0.269,  0.282,  0.268,  0.254,  0.239,  0.225,  0.211,\n",
       "        0.224,  0.236,  0.249,  0.261,  0.274,  0.277,  0.28 ,  0.282,\n",
       "        0.285,  0.288,  0.237,  0.186,  0.135,  0.084,  0.033,  0.077,\n",
       "        0.12 ,  0.164,  0.207,  0.251,  0.237,  0.223,  0.208,  0.194,\n",
       "        0.18 ,  0.144,  0.108,  0.072,  0.036,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.037,  0.074,  0.11 ,  0.147,  0.184,  0.18 ,  0.175,  0.171,\n",
       "        0.166,  0.162,  0.13 ,  0.097,  0.065,  0.032,  0.   ,  0.003,\n",
       "        0.007,  0.01 ,  0.014,  0.017,  0.141,  0.266,  0.39 ,  0.515,\n",
       "        0.639,  0.649,  0.659,  0.669,  0.679,  0.689,  0.677,  0.665,\n",
       "        0.654,  0.642,  0.63 ,  0.632,  0.634,  0.637,  0.639,  0.641,\n",
       "        0.627,  0.613,  0.598,  0.584,  0.57 ,  0.581,  0.592,  0.603,\n",
       "        0.614,  0.625,  0.615,  0.604,  0.594,  0.583,  0.573,  0.57 ,\n",
       "        0.567,  0.564,  0.561,  0.558,  0.56 ,  0.562,  0.564,  0.566,\n",
       "        0.568,  0.531,  0.494,  0.458,  0.421,  0.384,  0.344,  0.303,\n",
       "        0.263,  0.222,  0.182,  0.146,  0.109,  0.073,  0.036,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.008,  0.017,  0.025,  0.034,  0.042,  0.034,\n",
       "        0.025,  0.017,  0.008,  0.   ,  0.018,  0.036,  0.054,  0.072,\n",
       "        0.09 ,  0.077,  0.064,  0.052,  0.039,  0.026,  0.023,  0.02 ,\n",
       "        0.018,  0.015,  0.012,  0.029,  0.046,  0.062,  0.079,  0.096,\n",
       "        0.077,  0.058,  0.038,  0.019,  0.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
