{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('../data/test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('../data/trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.train_len = train_len\n",
    "        self.seq_len = seq_len\n",
    "        self.num_heads = num_heads\n",
    "        #self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        head_size = self.attention_size // self.num_heads\n",
    "        self.W_Q = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_K = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_V = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_O = nn.Linear(self.attention_size, self.attention_size)\n",
    "\n",
    "        \n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.attention_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.attention_size)\n",
    "        \n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.attention_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.attention_size)\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(self.attention_size, 1)\n",
    "        self.criterion = nn.MSELoss() # Switch to mean squared error instead of simple norm (this is better apparently?)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of size                                               (batch_size, sequence_length, embedding_size)\n",
    "        if x.shape[-1] != self.embed_size:\n",
    "            raise ValueError\n",
    "        \n",
    "\n",
    "        head_outputs = []\n",
    "        for head in range(self.num_heads):\n",
    "            queries = self.W_Q(x) #                                   (batch_size, sequence_length, attention_size)\n",
    "            keys = self.W_K(x) #                                      (batch_size, sequence_length, attention_size)\n",
    "            values = self.W_V(x) #                                    (batch_size, sequence_length, attention_size)\n",
    "\n",
    "            # Scale to prevent overflow errors, divide by square root of attention dimension\n",
    "            scale = torch.sqrt(torch.Tensor(queries.size(-1)))\n",
    "\n",
    "            #Compute attention and then normalize\n",
    "            attention = torch.bmm(queries, keys.transpose(1,2)) / scale #                  (batch_size, seq_len, seq_len)\n",
    "            weights = torch.nn.functional.softmax(attention, dim=2) # Apply this per sample\n",
    "\n",
    "            # Use as weights for values\n",
    "            context = torch.bmm(weights, values) #    (batch_size, attention_size, sequence_length)\n",
    "            head_outputs.append(context)\n",
    "\n",
    "        # Combine heads\n",
    "        multi_head = torch.cat(head_outputs, dim=-1)\n",
    "        attention_output = self.W_O(multi_head)\n",
    "\n",
    "        # Add first layernorm + residual (add initial info)\n",
    "        x = self.layer_norm1(x + attention_output)\n",
    "\n",
    "        # Run through all FFN layers\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Add second layernorm + residual\n",
    "        x = self.layer_norm2(x + ffn_output)\n",
    "\n",
    "        #Output projection\n",
    "        x = self.output_proj(x)\n",
    "\n",
    "        # Return prediction with added b term\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) # This is for stability\n",
    "        self.optimizer.step()\n",
    "        return loss.item() # Diagnostic info\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        losses = []\n",
    "        for epoch in range(self.train_len):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.train_len}, Loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m SelfAttentionFeedForward(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m295\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 113\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    111\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 113\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    115\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "Cell \u001b[0;32mIn[49], line 101\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 101\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, y)\n\u001b[1;32m    103\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[49], line 65\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m head_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads):\n\u001b[0;32m---> 65\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#                                   (batch_size, sequence_length, attention_size)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_K(x) \u001b[38;5;66;03m#                                      (batch_size, sequence_length, attention_size)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_V(x) \u001b[38;5;66;03m#                                    (batch_size, sequence_length, attention_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:394\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    396\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = DNADataset(embedded_dna_strands_train, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(20, 295, 5, 10, 1, 1e-4, 100, 1) # (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04466298,  0.04145521,  0.04282215, ...,  0.01897781,\n",
       "         0.01897781,  0.01897781],\n",
       "       [-0.0492821 ,  0.04089904,  0.04809882, ...,  0.01897781,\n",
       "         0.01897781,  0.01897781],\n",
       "       [ 0.01717338,  0.01775044,  0.01639129, ..., -0.05476579,\n",
       "        -0.05476579, -0.05476579],\n",
       "       ...,\n",
       "       [ 0.00803119,  0.01332299,  0.02351362, ..., -0.05476579,\n",
       "        -0.05476579, -0.05476579],\n",
       "       [-0.00102001, -0.00231577,  0.01801525, ..., -0.05476579,\n",
       "        -0.05476579, -0.05476579],\n",
       "       [ 0.01110825,  0.01059504,  0.01076886, ..., -0.05932143,\n",
       "        -0.05932143, -0.05932143]], shape=(7720, 295), dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.Tensor(embedded_dna_strands_test)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The output should now reflect predictions based on the training data\n",
    "\n",
    "#Generate predictions\n",
    "predictions = []\n",
    "for i, strand in enumerate(embedded_dna_strands_train):\n",
    "    logits = model.forward(torch.Tensor(strand).unsqueeze(0)).detach().numpy().flatten()\n",
    "    predicted_scores = torch.sigmoid(torch.Tensor(logits)).numpy().flatten()  #Apply Sigmoid to logits\n",
    "    predicted_labels = [\"A\" if score > 0.5 else \"R\" for score in predicted_scores]  #0.5 threshold\n",
    "    sequence_id = f\"train{str(i).zfill(4)}\"\n",
    "    nucleotides = raw_dna_strands_train[i]\n",
    "\n",
    "    for pos, (nucleotide, label) in enumerate(zip(nucleotides, predicted_labels)):\n",
    "        predictions.append([label, sequence_id, nucleotide])\n",
    "\n",
    "#Save predictions\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "predictions_df.to_csv(\"predictions.txt\", sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "#Load training solutions\n",
    "training_solutions = pd.read_csv(\"../data/trainsolutions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Load predictions\n",
    "predictions = pd.read_csv(\"predictions.txt\", delimiter=\"\\t\", header=None, names=[\"Activation/Repression\", \"Sequence_ID\", \"Nucleotide\"])\n",
    "\n",
    "#Ensure \"Nucleotide\" columns are strings in both DataFrames\n",
    "predictions[\"Nucleotide\"] = predictions[\"Nucleotide\"].astype(str)\n",
    "training_solutions[\"Nucleotide\"] = training_solutions[\"Nucleotide\"].astype(str)\n",
    "\n",
    "#Merge predictions and training solutions\n",
    "comparison = pd.merge(predictions, training_solutions, on=[\"Sequence_ID\", \"Nucleotide\"], how=\"inner\", suffixes=('_pred', '_true'))\n",
    "\n",
    "#Add a \"Match\" column to indicate where Prediction matches Truth\n",
    "comparison[\"Match\"] = comparison[\"Activation/Repression_pred\"] == comparison[\"Activation/Repression_true\"]\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = comparison[\"Match\"].mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "#Save the comparison to a file for inspection\n",
    "comparison.to_csv(\"comparison.txt\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
