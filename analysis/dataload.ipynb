{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/train_MPRA.txt', delimiter='\\t', header=None)\n",
    "test_raw = pd.read_csv('../data/test_MPRA.txt', delimiter='\\t', header=None)\n",
    "train_sol = pd.read_csv('../data/trainsolutions.txt', delimiter='\\t', header=None)\n",
    "train_raw.head()\n",
    "strand_length = 295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y data\n",
    "train_scores = np.array(train_raw.iloc[:, 2:297]) #Dimensions are 8000 (samples) by 295 (SHARPR scores per nucleotide)\n",
    "raw_dna_strands_train = [list(train_raw[1][i]) for i in range(len(train_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_train = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_train[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(train_raw))] #One hot encoded dna strands, list of 8000 matrices, each (295,5)\n",
    "embedded_dna_strands_train = [embedded_dna_strands_train[i] for i in range(len(embedded_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "train_scores  = [train_scores[i] for i in range(len(raw_dna_strands_train)) if not (\"N\" in raw_dna_strands_train[i])]\n",
    "#Repeat for test data\n",
    "raw_dna_strands_test = [list(test_raw[1][i]) for i in range(len(test_raw))] #List of lists holding DNA strands separated by character. Size 8000 lists each of length 290\n",
    "embedded_dna_strands_test = [np.column_stack((np.array(pd.get_dummies(pd.concat([pd.Series(raw_dna_strands_test[i]), pd.Series([\"A\", \"C\", \"T\", \"G\"])]), dtype='int'))[:-4], np.arange(295))) for i in range(len(test_raw))]\n",
    "embedded_dna_strands_test = [embedded_dna_strands_test[i] for i in range(len(embedded_dna_strands_test)) if not (\"N\" in raw_dna_strands_test[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_encoding(sequence_length, encoding_dim):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings for a sequence\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of the sequence\n",
    "        encoding_dim: Number of dimensions for positional encoding\n",
    "    \"\"\"\n",
    "    # Create position vector (0, 1, 2, ...)\n",
    "    position = torch.arange(sequence_length).unsqueeze(1)\n",
    "    \n",
    "    # Create scaling factor for different dimensions\n",
    "    div_term = torch.exp(torch.arange(0, encoding_dim, 2) * (-np.log(10000.0) / encoding_dim))\n",
    "    \n",
    "    # Create empty encoding matrix\n",
    "    pos_encoding = torch.zeros(sequence_length, encoding_dim)\n",
    "    \n",
    "    # Fill with sine and cosine values\n",
    "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# Convert your embeddings  # Your original embeddings\n",
    "pos_dim = 16  # Number of positional encoding dimensions\n",
    "\n",
    "# Convert to tensor if not already\n",
    "if not isinstance(embedded_dna_strands_train[0], torch.Tensor):\n",
    "    embedded_dna_strands_train = [torch.tensor(emb, dtype=torch.float32) for emb in embedded_dna_strands_train]\n",
    "\n",
    "# Get nucleotide part (first 4 dimensions)\n",
    "nucleotide_encodings = [emb[:, :4] for emb in embedded_dna_strands_train]\n",
    "\n",
    "# Create positional encodings for sequence length\n",
    "pos_encodings = create_positional_encoding(embedded_dna_strands_train[0].shape[0], pos_dim)\n",
    "\n",
    "# Combine nucleotide encodings with positional encodings\n",
    "final_embeddings = [torch.cat([nuc, pos_encodings], dim=1) for nuc in nucleotide_encodings]\n",
    "\n",
    "# Stack if you need them in a single tensor\n",
    "final_embeddings = torch.stack(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column with unique identifier for each nucleotide (sequence + location)\n",
    "train_sol[3] = [str(train_sol.iloc[i, 1][5:]).zfill(4) + str(train_sol.iloc[i,2]).zfill(3) for i in range(len(train_sol))]\n",
    "\n",
    "#Split by activators and repressors\n",
    "train_sol_act = train_sol[train_sol[0] == 'A'][3]\n",
    "train_sol_rep = train_sol[train_sol[0] == 'R'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, embedded_dna_strands, train_scores):\n",
    "        self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
    "        self.y = torch.tensor(train_scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionFeedForward(nn.Module):\n",
    "    #Initialize hyperparameters and NN matrices\n",
    "    def __init__(self, attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.train_len = train_len\n",
    "        self.seq_len = seq_len\n",
    "        self.num_heads = num_heads\n",
    "        #self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.initAttention()\n",
    "        self.initFFN()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    #Initialize our weight matrices as torch objects, allows them to be automatically optimized\n",
    "    def initAttention(self):\n",
    "        head_size = self.attention_size // self.num_heads\n",
    "        self.W_Q = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_K = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_V = nn.ModuleList([nn.Linear(self.embed_size, head_size, bias=True) for _ in range(self.num_heads)])\n",
    "        self.W_O = nn.Linear(self.attention_size, self.attention_size)\n",
    "        self.input_proj = nn.Linear(self.embed_size, self.attention_size)\n",
    "\n",
    "            # Xavier initialization\n",
    "        for layer in self.W_Q + self.W_K + self.W_V:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.xavier_uniform_(self.W_O.weight)\n",
    "\n",
    "        \n",
    "    \n",
    "    #Initialize Feed Forward layers, based on however many hidden layers we want\n",
    "    def initFFN(self):\n",
    "\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.attention_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.attention_size)\n",
    "        \n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.attention_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, self.attention_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.output_proj = nn.Linear(self.attention_size, 1)\n",
    "        self.criterion = nn.MSELoss() # Switch to mean squared error instead of simple norm (this is better apparently?)\n",
    "    \n",
    "    def custom_loss(self, pred, target):\n",
    "        mse = self.criterion(pred, target)\n",
    "        variance_penalty = -0.01 * torch.var(pred, dim=1).mean()  # Encourage variation\n",
    "        return mse + variance_penalty\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of size                                               (batch_size, sequence_length, embedding_size)\n",
    "        if x.shape[-1] != self.embed_size:\n",
    "            raise ValueError\n",
    "        \n",
    "        residual = self.input_proj(x)\n",
    "\n",
    "        head_outputs = []\n",
    "        for head in range(self.num_heads):\n",
    "            queries = self.W_Q[head](x) #                                   (batch_size, sequence_length, attention_size)\n",
    "            keys = self.W_K[head](x) #                                      (batch_size, sequence_length, attention_size)\n",
    "            values = self.W_V[head](x) #                                    (batch_size, sequence_length, attention_size)\n",
    "\n",
    "            # Scale to prevent overflow errors, divide by square root of attention dimension\n",
    "            scale = torch.sqrt(torch.Tensor([queries.size(-1)]))\n",
    "\n",
    "            #Compute attention and then normalize\n",
    "            attention = torch.bmm(queries, keys.transpose(1,2)) / scale #                  (batch_size, seq_len, seq_len)\n",
    "            weights = torch.nn.functional.dropout(torch.nn.functional.softmax(attention, dim=2), p=0.1, training=self.training) # Apply this per sample\n",
    "\n",
    "            # Use as weights for values\n",
    "            context = torch.bmm(weights, values) #    (batch_size, attention_size, sequence_length)\n",
    "            head_outputs.append(context)\n",
    "\n",
    "        # Combine heads\n",
    "        multi_head = torch.cat(head_outputs, dim=-1)\n",
    "        attention_output = self.W_O(multi_head)\n",
    "\n",
    "        # Add first layernorm + residual (add initial info)\n",
    "        x = self.layer_norm1(attention_output + residual)\n",
    "\n",
    "        # Run through all FFN layers\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Add second layernorm + residual\n",
    "        x = self.layer_norm2(ffn_output + residual)\n",
    "\n",
    "        #Output projection\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_proj(x)\n",
    "\n",
    "        # Return prediction with added b term\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self(x)\n",
    "        loss = self.custom_loss(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) # This is for stability\n",
    "        self.optimizer.step()\n",
    "        return loss.item() # Diagnostic info\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        losses = []\n",
    "        for epoch in range(self.train_len):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.train_len}, Loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_46795/784400942.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(embedded_dna_strands, dtype=torch.float32) # Convert x and y to tensors\n",
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_46795/784400942.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  self.y = torch.tensor(train_scores, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.0252\n",
      "Epoch 2/20, Loss: 0.3525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m SelfAttentionFeedForward(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m295\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m# (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len, num_heads)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    123\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 125\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    127\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "Cell \u001b[0;32mIn[8], line 113\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 113\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_loss(pred, y)\n\u001b[1;32m    115\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/Hackathon/MPRA_Challenge/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m, in \u001b[0;36mSelfAttentionFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \u001b[38;5;66;03m# Apply this per sample\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Use as weights for values\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#    (batch_size, attention_size, sequence_length)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     head_outputs\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Combine heads\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = DNADataset(final_embeddings, train_scores)\n",
    "\n",
    "# Create a DataLoader for batching, shuffling, and parallel data loading\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SelfAttentionFeedForward(60, 295, 20, 30, 1, 1e-1, 20, 5) # (attention_size, seq_len, embed_size, hidden_size, hidden_layers, lr, train_len, num_heads)\n",
    "model.train(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_predictions = model.forward(torch.Tensor(final_embeddings)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14745839,  0.00160678, -0.34630793, -0.07004087, -0.02158372,\n",
       "        0.04086192,  0.05970462,  0.09347995,  0.09696658,  0.10687621,\n",
       "        0.10852177,  0.10981353,  0.11047013,  0.11082776,  0.11095841,\n",
       "        0.11100657,  0.11104329,  0.11100514,  0.11102183,  0.11103423,\n",
       "        0.11102946,  0.11103041,  0.11101754,  0.11100753,  0.11100896,\n",
       "        0.11099847,  0.11100467,  0.11100371,  0.11099799,  0.11100133,\n",
       "        0.1110018 ,  0.11099561,  0.1109999 ,  0.11099608,  0.11099942,\n",
       "        0.11100037,  0.11099561,  0.1110018 ,  0.11099656,  0.11099608,\n",
       "        0.11099704,  0.11099608,  0.11099513,  0.11099751,  0.11099751,\n",
       "        0.11099561,  0.11099799,  0.11099608,  0.11099656,  0.1109937 ,\n",
       "        0.11099561,  0.11099561,  0.11099418,  0.11099418,  0.11099418,\n",
       "        0.11099561,  0.11099513,  0.11099465,  0.11099513,  0.11099561,\n",
       "        0.11099513,  0.11099513,  0.11099704,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656,\n",
       "        0.11099656,  0.11099656,  0.11099656,  0.11099656,  0.11099656],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_predictions.reshape(7720*295)\n",
    "r.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05639719, -0.05639719, -0.05639719, ..., -0.05638735,\n",
       "       -0.05638735, -0.05638735], shape=(2277390,), dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:-10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
